{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3itMzRIMbu57"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import concurrent.futures\n",
    "import nlpaug.augmenter.word as naw\n",
    "import tqdm  # Для прогресс-бара\n",
    "from transformers import TFXLMRobertaModel, XLMRobertaTokenizer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgErpLGId1PL"
   },
   "outputs": [],
   "source": [
    "# Инициализация лемматизатора\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia--nVeqd4_7"
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "data = pd.read_csv('emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDDDmLild88-"
   },
   "outputs": [],
   "source": [
    "# Загрузка стоп-слов\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFk3MDuXd_7F"
   },
   "outputs": [],
   "source": [
    "# Функция для предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Нормализация к нижнему регистру\n",
    "    text = re.sub(r'\\d+', '', text)  # Удаление цифр\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    words = [word for word in text.split() if word not in stop_words]  # Удаление стоп-слов\n",
    "    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]  # Лемматизация\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AMo0NWeeFEC"
   },
   "outputs": [],
   "source": [
    "# Применение предобработки\n",
    "data['Текст фразы'] = data['Текст фразы'].apply(preprocess_text)\n",
    "\n",
    "# Проверка результата\n",
    "print(data.head())\n",
    "\n",
    "# Сохранение обработанных данных\n",
    "data.to_csv('processed_emotions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG8PkTTGeUm-"
   },
   "outputs": [],
   "source": [
    "def augment_single_text(text, back_translation_aug):\n",
    "    try:\n",
    "        # Агментация текста с использованием перевода\n",
    "        return back_translation_aug.augment(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting text: {text[:30]}... Error: {e}\")\n",
    "        return text  # Возвращаем исходный текст, если произошла ошибка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60uadan0eX5t"
   },
   "outputs": [],
   "source": [
    "    back_translation_aug = naw.BackTranslationAug(\n",
    "        from_model_name='Helsinki-NLP/opus-mt-en-ru',\n",
    "        to_model_name='Helsinki-NLP/opus-mt-ru-en'\n",
    "    )\n",
    "\n",
    "    augmented_data = []\n",
    "\n",
    "    # Обрабатываем данные по частям (например, по 100 строк)\n",
    "    for i in tqdm.tqdm(range(0, len(data), chunk_size), desc=\"Processing data in chunks\"):\n",
    "        chunk = data.iloc[i:i+chunk_size]\n",
    "        # Используем ThreadPoolExecutor для параллельной обработки\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            for augmented_text in executor.map(lambda text: augment_single_text(text, back_translation_aug), chunk['Текст фразы']):\n",
    "                augmented_data.append(augmented_text)\n",
    "\n",
    "    data['Augmented'] = augmented_data\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Загружаем обработанные данные\n",
    "        data = pd.read_csv(r\"processed_emotions.csv\")\n",
    "        print(f\"Data loaded successfully. First few rows:\\n{data.head()}\")\n",
    "\n",
    "        # Применяем агментацию\n",
    "        new_data = augment_data(data, 'Недовольство', n_syn=10, n_back=5)\n",
    "\n",
    "        # Выводим результат\n",
    "        print(new_data.head())\n",
    "\n",
    "        # Сохраняем результаты в новый файл\n",
    "        new_data.to_csv(r\"augmented_emotions.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUZGjt4Per-s"
   },
   "outputs": [],
   "source": [
    "# Загружаем оба файла\n",
    "processed_data = pd.read_csv(\"emotions.csv\")\n",
    "augmented_data = pd.read_csv(\"augmented_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo9bqtrYetX6"
   },
   "outputs": [],
   "source": [
    "# Объединяем данные\n",
    "data = pd.concat([processed_data, augmented_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAdod8bGe4AX"
   },
   "outputs": [],
   "source": [
    "# Создаем аугментатор для замены синонимов\n",
    "aug = naw.SynonymAug(aug_p=0.2)  # aug_p - вероятность замены синонимов\n",
    "\n",
    "# Первая аугментация\n",
    "augmented_texts_1 = [aug.augment(text) for text in data['Текст фразы']]\n",
    "\n",
    "# Обновляем данные с первой аугментацией\n",
    "data['Текст фразы'] = augmented_texts_1\n",
    "\n",
    "# Вторая аугментация на уже аугментированных данных\n",
    "augmented_texts_2 = [aug.augment(text) for text in data['Текст фразы']]\n",
    "\n",
    "# Обновляем данные с второй аугментацией\n",
    "data['Текст фразы'] = augmented_texts_2\n",
    "\n",
    "# Сохраняем обновленные данные в новый файл\n",
    "data.to_csv(\"augmented_data.csv\", index=False)\n",
    "\n",
    "print(\"Двойная аугментация завершена. Данные сохранены в 'augmented_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kM0wScofM1D"
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "raw_data_path = \"emotions.csv\"\n",
    "augmented_data_path = \"augmented_data.csv\"\n",
    "\n",
    "raw_data = pd.read_csv(raw_data_path)\n",
    "augmented_data = pd.read_csv(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-d8nrabfYN8"
   },
   "outputs": [],
   "source": [
    "# Преобразование меток эмоций в числовой формат\n",
    "raw_data['Эмоция'] = raw_data['Эмоция'].astype('category').cat.codes\n",
    "augmented_data['Эмоция'] = augmented_data['Эмоция'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKcWWP--feVU"
   },
   "outputs": [],
   "source": [
    "# Инициализация токенизатора\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Разделение сырых данных на обучающую и тестовую выборки\n",
    "X_train_raw, X_test, y_train_raw, y_test = train_test_split(\n",
    "    raw_data['Текст фразы'], raw_data['Эмоция'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Объединение сырых и аугментированных данных для второго этапа\n",
    "combined_data = pd.concat([raw_data, augmented_data], ignore_index=True)\n",
    "X_train_combined = combined_data['Текст фразы']\n",
    "y_train_combined = combined_data['Эмоция']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pCX8J5Gfie-"
   },
   "outputs": [],
   "source": [
    "# Токенизация данных\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=64,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Загрузка модели XLM-RoBERTa для получения эмбеддингов\n",
    "embedding_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Заморозка первых шести слоёв модели\n",
    "for layer in embedding_model.roberta.encoder.layer[:6]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Оптимизация с использованием шедулера\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "embedding_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMWkIr0MfsxD"
   },
   "outputs": [],
   "source": [
    "# Получение эмбеддингов батчами\n",
    "def get_embeddings_batchwise(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        tokens = tokenize_texts(batch_texts)\n",
    "        batch_embeddings = embedding_model(tokens)['last_hidden_state'][:, 0, :].numpy()  # Первый токен [CLS]\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Главная функция\n",
    "if __name__ == \"__main__\":\n",
    "    # Первый этап: обучение на сырых данных\n",
    "    print(\"Этап 1: обучение на сырых данных...\")\n",
    "    train_embeddings_raw = get_embeddings_batchwise(X_train_raw)\n",
    "    test_embeddings = get_embeddings_batchwise(X_test)\n",
    "\n",
    "    svm = SVC(kernel='linear', C=1, probability=True)\n",
    "    svm.fit(train_embeddings_raw, y_train_raw)\n",
    "\n",
    "    # Оценка на тестовой выборке после первого этапа\n",
    "    svm_preds_raw = svm.predict(test_embeddings)\n",
    "    accuracy_raw = accuracy_score(y_test, svm_preds_raw)\n",
    "    print(f\"Точность на сырых данных: {accuracy_raw:.4f}\")\n",
    "\n",
    "    # Второй этап: дообучение на объединенных данных\n",
    "    print(\"Этап 2: дообучение на объединенных данных...\")\n",
    "    train_embeddings_combined = get_embeddings_batchwise(X_train_combined)\n",
    "    svm.fit(train_embeddings_combined, y_train_combined)\n",
    "\n",
    "    # Оценка на тестовой выборке после второго этапа\n",
    "    svm_preds_combined = svm.predict(test_embeddings)\n",
    "    accuracy_combined = accuracy_score(y_test, svm_preds_combined)\n",
    "    precision_combined = precision_score(y_test, svm_preds_combined, average='weighted')\n",
    "    recall_combined = recall_score(y_test, svm_preds_combined, average='weighted')\n",
    "    f1_combined = f1_score(y_test, svm_preds_combined, average='weighted')\n",
    "\n",
    "    # Вывод метрик\n",
    "    print(f\"Точность на объединенных данных: {accuracy_combined:.4f}\")\n",
    "    print(f\"Precision: {precision_combined:.4f}\")\n",
    "    print(f\"Recall: {recall_combined:.4f}\")\n",
    "    print(f\"F1-Score: {f1_combined:.4f}\")\n",
    "\n",
    "    # Сохранение модели SVM\n",
    "    model_path = \"svm_emotion_model.pkl\"\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(svm, f)\n",
    "    print(f\"Модель SVM сохранена в {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
